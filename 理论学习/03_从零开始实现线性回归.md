## 线性回归从零开始实现

### 1.处理输入数据和标签值

构造带噪声的人造数据集
```
def synthetic_data(w, b, num_examples):
    # 生成 y = wx + b + 噪声
    x = torch.normal(0, 1, size=(num_examples, len(w)))
    y = torch.matmul(x, w)+b # 计算labels值
    y += torch.normal(0, 0.01, size=y.shape) # 加上噪声
    return x, y.reshape(-1, 1) #返回训练集和labels集
    
true_x = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_x, true_b, 1000) # 构建1000个训练值
```
训练模型的过程就是无限逼近下面真实权值和偏置的过程

### 2. 构造线性权值和偏置初值:

```
w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)
```
torch.normal(mean, std, size)：
torch正态分布函数：mean表示元素的均值，std是元素的标准差

```
b = torch.zeros(1, requires_grad=True)
```
初始化偏置为0

### 3.构造模型

```
def Linear(x, w, b):
    #线性回归模型
    return torch.matmul(x, w)+b
```
该模型返回训练集矩阵与权值矩阵相乘并加上偏置的值
troch.matmul(m1, m2, ...)计算矩阵乘法的方法

### 4.把输入数据和标签指分批次

```
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i:min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
```
输入batch_size、训练集和标签集，可以将数据分为一个个batch_size大小的批样本
其中random.shuffle()函数会将训练集顺序随机打乱
yield是回一直返回分好的训练集和标签集，直到for循环结束

### 5.构造损失函数和优化器

```
#损失函数，使用方差
def squared_loss(y_train, y_valid):
    return(y_train - y_valid.reshape(y_train.shape))**2 / 2

  
#优化器
def sgd(params, lr, batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

```
batch_size = 10 #批处理大小
lr = 0.03 #学习率
num_epochs = 3 #周期数量
net = Linear #训练网络
loss = squared_loss #损失函数
```
这里我们先将需要自己设置的参数定义
### 6.开始训练

```
for epoch in range(num_epochs):
    for x,y in data_iter(batch_size, features, labels):
        l = loss(net(x, w, b), y)
        l.sum().backward()
        sgd([w, b], lr, batch_size)
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
 
print(f'w的估计误差:{true_w - w.reshape(true_w.shape)}')
print(f'b的估计误差:{true_b - b}')
```
运行后输出：
```
epoch 1, loss 0.033328
epoch 2, loss 0.000117
epoch 3, loss 0.000052

w的估计误差:tensor([ 0.0002, -0.0004], grad_fn=<SubBackward0>)
b的估计误差:tensor([2.3842e-06], grad_fn=<RsubBackward1>)
```
可以看到我们从零实现了一个线性回归模型
